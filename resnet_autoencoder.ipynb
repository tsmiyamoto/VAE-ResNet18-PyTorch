{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, mode='nearest'):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class BasicBlockEnc(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        planes = in_planes*stride\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if stride == 1:\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlockDec(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        planes = int(in_planes/stride)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        # self.bn1 could have been placed here, but that messes up the order of the layers when printing the class\n",
    "\n",
    "        if stride == 1:\n",
    "            self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.conv1 = ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn2(self.conv2(x)))\n",
    "        out = self.bn1(self.conv1(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18Enc(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.z_dim = z_dim\n",
    "        self.conv1 = nn.Conv2d(nc, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlockEnc, 64, num_Blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlockEnc, 128, num_Blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlockEnc, 256, num_Blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlockEnc, 512, num_Blocks[3], stride=2)\n",
    "\n",
    "    def _make_layer(self, BasicBlockEnc, planes, num_Blocks, stride):\n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers += [BasicBlockEnc(self.in_planes, stride)]\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "#         x = F.adaptive_avg_pool2d(x, 1)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.linear(x)\n",
    "#         mu = x[:, :self.z_dim]\n",
    "#         logvar = x[:, self.z_dim:]\n",
    "#         return mu, logvar\n",
    "        return x\n",
    "\n",
    "class ResNet18Dec(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = 512\n",
    "\n",
    "\n",
    "        self.layer4 = self._make_layer(BasicBlockDec, 512, num_Blocks[3], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlockDec, 256, num_Blocks[2], stride=2)\n",
    "        self.layer2 = self._make_layer(BasicBlockDec, 128, num_Blocks[1], stride=2)\n",
    "        self.layer1 = self._make_layer(BasicBlockDec, 64, num_Blocks[0], stride=1)\n",
    "        self.conv1 = ResizeConv2d(64, nc, kernel_size=3, scale_factor=2)\n",
    "        self.convtrans1 = nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def _make_layer(self, BasicBlockDec, planes, num_Blocks, stride):\n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in reversed(strides):\n",
    "            layers += [BasicBlockDec(self.in_planes, stride)]\n",
    "        self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "#         x = self.linear(z)\n",
    "#         x = x.view(z.size(0), 512, 1, 1)\n",
    "#         x = F.interpolate(z, scale_factor=4)\n",
    "        x = self.layer4(z)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer1(x)\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "#         x = self.convtrans1(x)\n",
    "        x = x.view(x.size(0), 3, 256, 256)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNet18Enc(z_dim=z_dim)\n",
    "        self.decoder = ResNet18Dec(z_dim=z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         mean, logvar = self.encoder(x)\n",
    "        z = self.encoder(x)\n",
    "#         z = self.reparameterize(mean, logvar)\n",
    "        x = self.decoder(z)\n",
    "        return x, z\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def reparameterize(mean, logvar):\n",
    "#         std = torch.exp(logvar / 2) # in log-space, squareroot is divide by two\n",
    "#         epsilon = torch.randn_like(std)\n",
    "#         return epsilon * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(z_dim=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ResNet18Enc: 1, Conv2d: 2, BatchNorm2d: 2, MaxPool2d: 2, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockDec: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, BasicBlockDec: 3, Conv2d: 4, BatchNorm2d: 4, ResizeConv2d: 4, Conv2d: 5, BatchNorm2d: 4, Sequential: 4, ResizeConv2d: 5, Conv2d: 6, BatchNorm2d: 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-87-0e942b9caab3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m#         z = self.reparameterize(mean, logvar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-87-0e942b9caab3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-87-0e942b9caab3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 512, 3, 3], expected input[1, 256, 16, 16] to have 512 channels, but got 256 channels instead",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-0ccb9291d70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     summary_list = forward_pass(\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0mformatting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFormattingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         ) from e\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ResNet18Enc: 1, Conv2d: 2, BatchNorm2d: 2, MaxPool2d: 2, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockDec: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, BasicBlockDec: 3, Conv2d: 4, BatchNorm2d: 4, ResizeConv2d: 4, Conv2d: 5, BatchNorm2d: 4, Sequential: 4, ResizeConv2d: 5, Conv2d: 6, BatchNorm2d: 5]"
     ]
    }
   ],
   "source": [
    "summary(vae, (1, 3, 256, 256), depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, mode='nearest'):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class BasicBlockEnc(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        planes = in_planes*stride\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if stride == 1:\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlockDec(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convtrans1 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=4)\n",
    "\n",
    "        planes = int(in_planes/stride)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        # self.bn1 could have been placed here, but that messes up the order of the layers when printing the class\n",
    " \n",
    "        if stride == 1:\n",
    "            self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.conv1 = ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn2(self.conv2(x)))\n",
    "        out = self.bn1(self.conv1(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18Enc(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.z_dim = z_dim\n",
    "        self.conv1 = nn.Conv2d(nc, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlockEnc, 64, num_Blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlockEnc, 128, num_Blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlockEnc, 256, num_Blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlockEnc, 512, num_Blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, 2 * z_dim)\n",
    "\n",
    "    def _make_layer(self, BasicBlockEnc, planes, num_Blocks, stride):\n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers += [BasicBlockEnc(self.in_planes, stride)]\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "#         x = F.adaptive_avg_pool2d(x, 1)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.linear(x)\n",
    "#         mu = x[:, :self.z_dim]\n",
    "#         logvar = x[:, self.z_dim:]\n",
    "#         return mu, logvar\n",
    "        return x\n",
    "\n",
    "class ResNet18Dec(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = 256\n",
    "\n",
    "        self.linear = nn.Linear(z_dim, 512)\n",
    "\n",
    "        self.layer4 = self._make_layer(BasicBlockDec, 256, num_Blocks[3], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlockDec, 128, num_Blocks[2], stride=2)\n",
    "        self.layer2 = self._make_layer(BasicBlockDec, 64, num_Blocks[1], stride=2)\n",
    "        self.layer1 = self._make_layer(BasicBlockDec, 32, num_Blocks[0], stride=1)\n",
    "        self.conv1 = ResizeConv2d(64, nc, kernel_size=3, scale_factor=2)\n",
    "\n",
    "    def _make_layer(self, BasicBlockDec, planes, num_Blocks, stride):\n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in reversed(strides):\n",
    "            layers += [BasicBlockDec(self.in_planes, stride)]\n",
    "        self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "#         x = self.linear(z)\n",
    "#         x = x.view(z.size(0), 512, 1, 1)\n",
    "#         x = F.interpolate(z, scale_factor=4)\n",
    "        x = self.layer4(z)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer1(x)\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = x.view(x.size(0), 3, INPUT_SHAPE, INPUT_SHAPE)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNet18Enc(z_dim=z_dim)\n",
    "        self.decoder = ResNet18Dec(z_dim=z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         mean, logvar = self.encoder(x)\n",
    "        z = self.encoder(x)\n",
    "#         z = self.reparameterize(mean, logvar)\n",
    "        x = self.decoder(z)\n",
    "        return x, z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ResNet18Enc: 1, Conv2d: 2, BatchNorm2d: 2, MaxPool2d: 2, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-147-a9c8935cb8f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m#         z = self.reparameterize(mean, logvar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-147-a9c8935cb8f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m#         x = F.interpolate(z, scale_factor=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-147-a9c8935cb8f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 64, 3, 3], expected input[1, 512, 8, 8] to have 64 channels, but got 512 channels instead",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-8c79550001c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     summary_list = forward_pass(\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0mformatting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFormattingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         ) from e\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ResNet18Enc: 1, Conv2d: 2, BatchNorm2d: 2, MaxPool2d: 2, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Sequential: 2, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, BasicBlockEnc: 3, Conv2d: 4, BatchNorm2d: 4, Conv2d: 4, BatchNorm2d: 4, Sequential: 4]"
     ]
    }
   ],
   "source": [
    "vae = VAE(z_dim=10).cuda()\n",
    "summary(vae, (1, 3, INPUT_SHAPE, INPUT_SHAPE), depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet18Enc                              --                        --\n",
       "├─Conv2d: 1-1                            [1, 64, 128, 128]         1,728\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 128, 128]         128\n",
       "├─MaxPool2d: 1-3                         [1, 64, 64, 64]           --\n",
       "├─Sequential: 1-4                        [1, 64, 64, 64]           --\n",
       "│    └─BasicBlockEnc: 2-1                [1, 64, 64, 64]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 64, 64]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 64, 64]           128\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 64, 64]           36,864\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 64, 64, 64]           128\n",
       "│    │    └─Sequential: 3-5              [1, 64, 64, 64]           --\n",
       "│    └─BasicBlockEnc: 2-2                [1, 64, 64, 64]           --\n",
       "│    │    └─Conv2d: 3-6                  [1, 64, 64, 64]           36,864\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 64, 64, 64]           128\n",
       "│    │    └─Conv2d: 3-8                  [1, 64, 64, 64]           36,864\n",
       "│    │    └─BatchNorm2d: 3-9             [1, 64, 64, 64]           128\n",
       "│    │    └─Sequential: 3-10             [1, 64, 64, 64]           --\n",
       "├─Sequential: 1-5                        [1, 128, 32, 32]          --\n",
       "│    └─BasicBlockEnc: 2-3                [1, 128, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-11                 [1, 128, 32, 32]          73,728\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 128, 32, 32]          256\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 32, 32]          147,456\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 32, 32]          256\n",
       "│    │    └─Sequential: 3-15             [1, 128, 32, 32]          --\n",
       "│    │    │    └─Conv2d: 4-1             [1, 128, 32, 32]          8,192\n",
       "│    │    │    └─BatchNorm2d: 4-2        [1, 128, 32, 32]          256\n",
       "│    └─BasicBlockEnc: 2-4                [1, 128, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 32, 32]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 32, 32]          256\n",
       "│    │    └─Conv2d: 3-18                 [1, 128, 32, 32]          147,456\n",
       "│    │    └─BatchNorm2d: 3-19            [1, 128, 32, 32]          256\n",
       "│    │    └─Sequential: 3-20             [1, 128, 32, 32]          --\n",
       "├─Sequential: 1-6                        [1, 256, 16, 16]          --\n",
       "│    └─BasicBlockEnc: 2-5                [1, 256, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-21                 [1, 256, 16, 16]          294,912\n",
       "│    │    └─BatchNorm2d: 3-22            [1, 256, 16, 16]          512\n",
       "│    │    └─Conv2d: 3-23                 [1, 256, 16, 16]          589,824\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 256, 16, 16]          512\n",
       "│    │    └─Sequential: 3-25             [1, 256, 16, 16]          --\n",
       "│    │    │    └─Conv2d: 4-3             [1, 256, 16, 16]          32,768\n",
       "│    │    │    └─BatchNorm2d: 4-4        [1, 256, 16, 16]          512\n",
       "│    └─BasicBlockEnc: 2-6                [1, 256, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 16, 16]          589,824\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 16, 16]          512\n",
       "│    │    └─Conv2d: 3-28                 [1, 256, 16, 16]          589,824\n",
       "│    │    └─BatchNorm2d: 3-29            [1, 256, 16, 16]          512\n",
       "│    │    └─Sequential: 3-30             [1, 256, 16, 16]          --\n",
       "├─Sequential: 1-7                        [1, 512, 8, 8]            --\n",
       "│    └─BasicBlockEnc: 2-7                [1, 512, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-31                 [1, 512, 8, 8]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 512, 8, 8]            1,024\n",
       "│    │    └─Conv2d: 3-33                 [1, 512, 8, 8]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 512, 8, 8]            1,024\n",
       "│    │    └─Sequential: 3-35             [1, 512, 8, 8]            --\n",
       "│    │    │    └─Conv2d: 4-5             [1, 512, 8, 8]            131,072\n",
       "│    │    │    └─BatchNorm2d: 4-6        [1, 512, 8, 8]            1,024\n",
       "│    └─BasicBlockEnc: 2-8                [1, 512, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-36                 [1, 512, 8, 8]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 512, 8, 8]            1,024\n",
       "│    │    └─Conv2d: 3-38                 [1, 512, 8, 8]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 512, 8, 8]            1,024\n",
       "│    │    └─Sequential: 3-40             [1, 512, 8, 8]            --\n",
       "==========================================================================================\n",
       "Total params: 11,168,832\n",
       "Trainable params: 11,168,832\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.24\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 51.90\n",
       "Params size (MB): 44.68\n",
       "Estimated Total Size (MB): 97.37\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = ResNet18Enc().cuda()\n",
    "summary(enc, (1, 3, 256, 256), depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlockDec(nn.Module):\n",
    "\n",
    "    def __init__(self, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convtrans1 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.convtrans2 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.convtrans1(x)\n",
    "        out2 = torch.relu(self.bn2(out1))\n",
    "        out2 = self.convtrans2(out2)\n",
    "        out2 = torch.relu(self.bn2(out2))\n",
    "        final = torch.add(out1, out2)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.BasicBlockDec'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BasicBlockDec                            --                        --\n",
       "├─ConvTranspose2d: 1-1                   [1, 512, 16, 16]          4,194,816\n",
       "├─BatchNorm2d: 1-2                       [1, 512, 16, 16]          1,024\n",
       "├─ConvTranspose2d: 1-3                   [1, 512, 16, 16]          2,359,808\n",
       "├─BatchNorm2d: 1-4                       [1, 512, 16, 16]          (recursive)\n",
       "==========================================================================================\n",
       "Total params: 6,555,648\n",
       "Trainable params: 6,555,648\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 3.15\n",
       "Params size (MB): 26.22\n",
       "Estimated Total Size (MB): 29.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic = BasicBlockDec().cuda()\n",
    "print(type(basic))\n",
    "summary(basic, (1, 512, 8, 8), depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, mode='nearest'):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class BasicBlockEnc(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        planes = in_planes*stride\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if stride == 1:\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlockDec(nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        if shape == 512:\n",
    "            shape2 = 512\n",
    "        else:\n",
    "            shape2 = int(shape * 2)\n",
    "        \n",
    "        self.convtrans1 = nn.ConvTranspose2d(shape2, shape, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(shape)\n",
    "        self.convtrans2 = nn.ConvTranspose2d(shape, shape, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.convtrans1(x)\n",
    "        out2 = torch.relu(self.bn2(out1))\n",
    "        out2 = self.convtrans2(out2)\n",
    "        out2 = torch.relu(self.bn2(out2))\n",
    "        final = torch.add(out1, out2)\n",
    "        \n",
    "        return final\n",
    "\n",
    "class ResNet18Enc(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.z_dim = z_dim\n",
    "        self.conv1 = nn.Conv2d(nc, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlockEnc, 64, num_Blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlockEnc, 128, num_Blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlockEnc, 256, num_Blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlockEnc, 512, num_Blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, 2 * z_dim)\n",
    "\n",
    "    def _make_layer(self, BasicBlockEnc, planes, num_Blocks, stride):\n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers += [BasicBlockEnc(self.in_planes, stride)]\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "#         x = F.adaptive_avg_pool2d(x, 1)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.linear(x)\n",
    "#         mu = x[:, :self.z_dim]\n",
    "#         logvar = x[:, self.z_dim:]\n",
    "#         return mu, logvar\n",
    "        return x\n",
    "\n",
    "class ResNet18Dec(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3):\n",
    "        super().__init__()\n",
    "        self.layer1 = BasicBlockDec(512)\n",
    "        self.layer2 = BasicBlockDec(256)\n",
    "        self.layer3 = BasicBlockDec(128)\n",
    "        self.layer4 = BasicBlockDec(64)\n",
    "\n",
    "        self.conv1 = ResizeConv2d(64, nc, kernel_size=3, scale_factor=2)\n",
    "\n",
    "    def _make_layer(self, BasicBlockDec, shape):\n",
    "        return \n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in reversed(strides):\n",
    "            layers += [BasicBlockDec(self.in_planes, stride)]\n",
    "        self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.linear(z)\n",
    "#         x = x.view(z.size(0), 512, 1, 1)\n",
    "#         x = F.interpolate(z, scale_factor=4)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "#         x = torch.sigmoid(self.conv1(x))\n",
    "#         x = x.view(x.size(0), 3, INPUT_SHAPE, INPUT_SHAPE)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNet18Enc(z_dim=z_dim)\n",
    "        self.decoder = ResNet18Dec(z_dim=z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         mean, logvar = self.encoder(x)\n",
    "        x = self.encoder(x)\n",
    "#         z = self.reparameterize(mean, logvar)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet18Dec                              --                        --\n",
       "├─BasicBlockDec: 1-1                     [1, 512, 16, 16]          --\n",
       "│    └─ConvTranspose2d: 2-1              [1, 512, 16, 16]          4,194,816\n",
       "│    └─BatchNorm2d: 2-2                  [1, 512, 16, 16]          1,024\n",
       "│    └─ConvTranspose2d: 2-3              [1, 512, 16, 16]          2,359,808\n",
       "│    └─BatchNorm2d: 2-4                  [1, 512, 16, 16]          (recursive)\n",
       "├─BasicBlockDec: 1-2                     [1, 256, 32, 32]          --\n",
       "│    └─ConvTranspose2d: 2-5              [1, 256, 32, 32]          2,097,408\n",
       "│    └─BatchNorm2d: 2-6                  [1, 256, 32, 32]          512\n",
       "│    └─ConvTranspose2d: 2-7              [1, 256, 32, 32]          590,080\n",
       "│    └─BatchNorm2d: 2-8                  [1, 256, 32, 32]          (recursive)\n",
       "├─BasicBlockDec: 1-3                     [1, 128, 64, 64]          --\n",
       "│    └─ConvTranspose2d: 2-9              [1, 128, 64, 64]          524,416\n",
       "│    └─BatchNorm2d: 2-10                 [1, 128, 64, 64]          256\n",
       "│    └─ConvTranspose2d: 2-11             [1, 128, 64, 64]          147,584\n",
       "│    └─BatchNorm2d: 2-12                 [1, 128, 64, 64]          (recursive)\n",
       "├─BasicBlockDec: 1-4                     [1, 64, 128, 128]         --\n",
       "│    └─ConvTranspose2d: 2-13             [1, 64, 128, 128]         131,136\n",
       "│    └─BatchNorm2d: 2-14                 [1, 64, 128, 128]         128\n",
       "│    └─ConvTranspose2d: 2-15             [1, 64, 128, 128]         36,928\n",
       "│    └─BatchNorm2d: 2-16                 [1, 64, 128, 128]         (recursive)\n",
       "==========================================================================================\n",
       "Total params: 10,084,096\n",
       "Trainable params: 10,084,096\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 9.94\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 47.19\n",
       "Params size (MB): 40.34\n",
       "Estimated Total Size (MB): 87.65\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = ResNet18Dec().cuda()\n",
    "summary(dec, (1, 512, 8, 8), depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VAE                                           --                        --\n",
       "├─ResNet18Enc: 1-1                            [1, 512, 8, 8]            --\n",
       "│    └─Conv2d: 2-1                            [1, 64, 128, 128]         1,728\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 128, 128]         128\n",
       "│    └─MaxPool2d: 2-3                         [1, 64, 64, 64]           --\n",
       "│    └─Sequential: 2-4                        [1, 64, 64, 64]           --\n",
       "│    │    └─BasicBlockEnc: 3-1                [1, 64, 64, 64]           --\n",
       "│    │    │    └─Conv2d: 4-1                  [1, 64, 64, 64]           36,864\n",
       "│    │    │    └─BatchNorm2d: 4-2             [1, 64, 64, 64]           128\n",
       "│    │    │    └─Conv2d: 4-3                  [1, 64, 64, 64]           36,864\n",
       "│    │    │    └─BatchNorm2d: 4-4             [1, 64, 64, 64]           128\n",
       "│    │    │    └─Sequential: 4-5              [1, 64, 64, 64]           --\n",
       "│    │    └─BasicBlockEnc: 3-2                [1, 64, 64, 64]           --\n",
       "│    │    │    └─Conv2d: 4-6                  [1, 64, 64, 64]           36,864\n",
       "│    │    │    └─BatchNorm2d: 4-7             [1, 64, 64, 64]           128\n",
       "│    │    │    └─Conv2d: 4-8                  [1, 64, 64, 64]           36,864\n",
       "│    │    │    └─BatchNorm2d: 4-9             [1, 64, 64, 64]           128\n",
       "│    │    │    └─Sequential: 4-10             [1, 64, 64, 64]           --\n",
       "│    └─Sequential: 2-5                        [1, 128, 32, 32]          --\n",
       "│    │    └─BasicBlockEnc: 3-3                [1, 128, 32, 32]          --\n",
       "│    │    │    └─Conv2d: 4-11                 [1, 128, 32, 32]          73,728\n",
       "│    │    │    └─BatchNorm2d: 4-12            [1, 128, 32, 32]          256\n",
       "│    │    │    └─Conv2d: 4-13                 [1, 128, 32, 32]          147,456\n",
       "│    │    │    └─BatchNorm2d: 4-14            [1, 128, 32, 32]          256\n",
       "│    │    │    └─Sequential: 4-15             [1, 128, 32, 32]          --\n",
       "│    │    │    │    └─Conv2d: 5-1             [1, 128, 32, 32]          8,192\n",
       "│    │    │    │    └─BatchNorm2d: 5-2        [1, 128, 32, 32]          256\n",
       "│    │    └─BasicBlockEnc: 3-4                [1, 128, 32, 32]          --\n",
       "│    │    │    └─Conv2d: 4-16                 [1, 128, 32, 32]          147,456\n",
       "│    │    │    └─BatchNorm2d: 4-17            [1, 128, 32, 32]          256\n",
       "│    │    │    └─Conv2d: 4-18                 [1, 128, 32, 32]          147,456\n",
       "│    │    │    └─BatchNorm2d: 4-19            [1, 128, 32, 32]          256\n",
       "│    │    │    └─Sequential: 4-20             [1, 128, 32, 32]          --\n",
       "│    └─Sequential: 2-6                        [1, 256, 16, 16]          --\n",
       "│    │    └─BasicBlockEnc: 3-5                [1, 256, 16, 16]          --\n",
       "│    │    │    └─Conv2d: 4-21                 [1, 256, 16, 16]          294,912\n",
       "│    │    │    └─BatchNorm2d: 4-22            [1, 256, 16, 16]          512\n",
       "│    │    │    └─Conv2d: 4-23                 [1, 256, 16, 16]          589,824\n",
       "│    │    │    └─BatchNorm2d: 4-24            [1, 256, 16, 16]          512\n",
       "│    │    │    └─Sequential: 4-25             [1, 256, 16, 16]          --\n",
       "│    │    │    │    └─Conv2d: 5-3             [1, 256, 16, 16]          32,768\n",
       "│    │    │    │    └─BatchNorm2d: 5-4        [1, 256, 16, 16]          512\n",
       "│    │    └─BasicBlockEnc: 3-6                [1, 256, 16, 16]          --\n",
       "│    │    │    └─Conv2d: 4-26                 [1, 256, 16, 16]          589,824\n",
       "│    │    │    └─BatchNorm2d: 4-27            [1, 256, 16, 16]          512\n",
       "│    │    │    └─Conv2d: 4-28                 [1, 256, 16, 16]          589,824\n",
       "│    │    │    └─BatchNorm2d: 4-29            [1, 256, 16, 16]          512\n",
       "│    │    │    └─Sequential: 4-30             [1, 256, 16, 16]          --\n",
       "│    └─Sequential: 2-7                        [1, 512, 8, 8]            --\n",
       "│    │    └─BasicBlockEnc: 3-7                [1, 512, 8, 8]            --\n",
       "│    │    │    └─Conv2d: 4-31                 [1, 512, 8, 8]            1,179,648\n",
       "│    │    │    └─BatchNorm2d: 4-32            [1, 512, 8, 8]            1,024\n",
       "│    │    │    └─Conv2d: 4-33                 [1, 512, 8, 8]            2,359,296\n",
       "│    │    │    └─BatchNorm2d: 4-34            [1, 512, 8, 8]            1,024\n",
       "│    │    │    └─Sequential: 4-35             [1, 512, 8, 8]            --\n",
       "│    │    │    │    └─Conv2d: 5-5             [1, 512, 8, 8]            131,072\n",
       "│    │    │    │    └─BatchNorm2d: 5-6        [1, 512, 8, 8]            1,024\n",
       "│    │    └─BasicBlockEnc: 3-8                [1, 512, 8, 8]            --\n",
       "│    │    │    └─Conv2d: 4-36                 [1, 512, 8, 8]            2,359,296\n",
       "│    │    │    └─BatchNorm2d: 4-37            [1, 512, 8, 8]            1,024\n",
       "│    │    │    └─Conv2d: 4-38                 [1, 512, 8, 8]            2,359,296\n",
       "│    │    │    └─BatchNorm2d: 4-39            [1, 512, 8, 8]            1,024\n",
       "│    │    │    └─Sequential: 4-40             [1, 512, 8, 8]            --\n",
       "├─ResNet18Dec: 1-2                            [1, 64, 128, 128]         --\n",
       "│    └─BasicBlockDec: 2-8                     [1, 512, 16, 16]          --\n",
       "│    │    └─ConvTranspose2d: 3-9              [1, 512, 16, 16]          4,194,816\n",
       "│    │    └─BatchNorm2d: 3-10                 [1, 512, 16, 16]          1,024\n",
       "│    │    └─ConvTranspose2d: 3-11             [1, 512, 16, 16]          2,359,808\n",
       "│    │    └─BatchNorm2d: 3-12                 [1, 512, 16, 16]          (recursive)\n",
       "│    └─BasicBlockDec: 2-9                     [1, 256, 32, 32]          --\n",
       "│    │    └─ConvTranspose2d: 3-13             [1, 256, 32, 32]          2,097,408\n",
       "│    │    └─BatchNorm2d: 3-14                 [1, 256, 32, 32]          512\n",
       "│    │    └─ConvTranspose2d: 3-15             [1, 256, 32, 32]          590,080\n",
       "│    │    └─BatchNorm2d: 3-16                 [1, 256, 32, 32]          (recursive)\n",
       "│    └─BasicBlockDec: 2-10                    [1, 128, 64, 64]          --\n",
       "│    │    └─ConvTranspose2d: 3-17             [1, 128, 64, 64]          524,416\n",
       "│    │    └─BatchNorm2d: 3-18                 [1, 128, 64, 64]          256\n",
       "│    │    └─ConvTranspose2d: 3-19             [1, 128, 64, 64]          147,584\n",
       "│    │    └─BatchNorm2d: 3-20                 [1, 128, 64, 64]          (recursive)\n",
       "│    └─BasicBlockDec: 2-11                    [1, 64, 128, 128]         --\n",
       "│    │    └─ConvTranspose2d: 3-21             [1, 64, 128, 128]         131,136\n",
       "│    │    └─BatchNorm2d: 3-22                 [1, 64, 128, 128]         128\n",
       "│    │    └─ConvTranspose2d: 3-23             [1, 64, 128, 128]         36,928\n",
       "│    │    └─BatchNorm2d: 3-24                 [1, 64, 128, 128]         (recursive)\n",
       "===============================================================================================\n",
       "Total params: 21,252,928\n",
       "Trainable params: 21,252,928\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.18\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 99.09\n",
       "Params size (MB): 85.01\n",
       "Estimated Total Size (MB): 184.89\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(z_dim=10).cuda()\n",
    "summary(vae, (1, 3, INPUT_SHAPE, INPUT_SHAPE), depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
